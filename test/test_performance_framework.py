#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@ProjectName: Homalos_v2
@FileName   : test_performance_framework.py
@Date       : 2025/1/15 16:30
@Author     : Donny
@Email      : donnymoving@gmail.com
@Software   : PyCharm
@Description: æ€§èƒ½æµ‹è¯•æ¡†æ¶ - ç«¯åˆ°ç«¯ã€å‹åŠ›æµ‹è¯•å’ŒåŸºå‡†æµ‹è¯•
"""
import asyncio
import statistics
import time
import uuid
from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Any, Optional

import psutil
import pytest

from src.config.config_manager import ConfigManager
from src.config.constant import Direction, OrderType, Exchange
from src.core.event import Event, create_trading_event
from src.core.event_bus import EventBus
from src.core.logger import get_logger
from src.core.object import OrderRequest, TickData
from src.services.data_service import DataService
from src.services.performance_monitor import PerformanceMonitor
from src.services.trading_engine import TradingEngine

logger = get_logger("PerformanceTest")


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    test_name: str
    success: bool
    duration: float
    metrics: Dict[str, Any] = field(default_factory=dict)
    error_message: Optional[str] = None


@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    operation: str
    total_operations: int
    duration: float
    operations_per_second: float
    avg_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    error_count: int
    success_rate: float


class PerformanceTestFramework:
    """æ€§èƒ½æµ‹è¯•æ¡†æ¶"""
    
    def __init__(self) -> None:
        # æµ‹è¯•é…ç½® - ä½¿ç”¨ä¸“ç”¨æµ‹è¯•é…ç½®æ–‡ä»¶
        self.test_config = {
            "event_bus_name": "test_trading_system",
            "config_file": "config/test_system.yaml"
        }
        
        # ç»„ä»¶
        self.event_bus: Optional[EventBus] = None
        self.config: Optional[ConfigManager] = None
        self.trading_engine: Optional[TradingEngine] = None
        self.data_service: Optional[DataService] = None
        self.performance_monitor: Optional[PerformanceMonitor] = None
        
        # æµ‹è¯•ç»“æœ
        self.test_results: List[TestResult] = []
        self.benchmark_results: List[BenchmarkResult] = []
        
        logger.info("æ€§èƒ½æµ‹è¯•æ¡†æ¶åˆå§‹åŒ–å®Œæˆ")
    
    async def setup_test_environment(self) -> bool:
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ - å¢å¼ºç‰ˆ"""
        try:
            logger.info("ğŸ”§ è®¾ç½®æ€§èƒ½æµ‹è¯•ç¯å¢ƒ...")
            
            # åˆå§‹åŒ–åŸºç¡€ç»„ä»¶
            self.config = ConfigManager(self.test_config["config_file"])
            self.event_bus = EventBus(name=self.test_config["event_bus_name"])
            
            # å¯åŠ¨äº‹ä»¶æ€»çº¿
            self.event_bus.start()
            
            # ç­‰å¾…äº‹ä»¶æ€»çº¿å®Œå…¨å¯åŠ¨
            await asyncio.sleep(0.5)
            
            # åˆå§‹åŒ–æ ¸å¿ƒæœåŠ¡
            self.trading_engine = TradingEngine(self.event_bus, self.config)
            self.data_service = DataService(self.event_bus, self.config)
            self.performance_monitor = PerformanceMonitor(self.event_bus, self.config)
            
            # åˆå§‹åŒ–æœåŠ¡ - æ·»åŠ é‡è¯•æœºåˆ¶
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    await self.trading_engine.initialize()
                    await self.data_service.initialize()
                    break
                except Exception as e:
                    if attempt == max_retries - 1:
                        raise
                    logger.warning(f"åˆå§‹åŒ–å¤±è´¥ï¼Œé‡è¯• {attempt + 1}/{max_retries}: {e}")
                    await asyncio.sleep(1)
            
            # å¯åŠ¨æœåŠ¡
            await self.trading_engine.start()
            self.performance_monitor.start_monitoring()
            
            # éªŒè¯ç»„ä»¶çŠ¶æ€
            if not self._verify_components_ready():
                raise Exception("ç»„ä»¶æœªæ­£ç¡®åˆå§‹åŒ–")
            
            logger.info("âœ… æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•ç¯å¢ƒè®¾ç½®å¤±è´¥: {e}")
            return False
    
    def _verify_components_ready(self) -> bool:
        """éªŒè¯ç»„ä»¶å°±ç»ªçŠ¶æ€"""
        checks = [
            self.event_bus is not None,
            self.trading_engine is not None,
            self.data_service is not None,
            self.performance_monitor is not None
        ]
        return all(checks)
    
    async def teardown_test_environment(self) -> None:
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        try:
            logger.info("ğŸ§¹ æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")
            
            if self.performance_monitor:
                self.performance_monitor.stop_monitoring()
            
            if self.trading_engine:
                await self.trading_engine.stop()
            
            if self.data_service:
                await self.data_service.shutdown()
            
            if self.event_bus:
                self.event_bus.stop()
            
            logger.info("âœ… æµ‹è¯•ç¯å¢ƒæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ æµ‹è¯•ç¯å¢ƒæ¸…ç†å¤±è´¥: {e}")
    
    async def run_end_to_end_test(self) -> TestResult:
        """ç«¯åˆ°ç«¯æµ‹è¯• - å®Œæ•´äº¤æ˜“æµç¨‹"""
        test_name = "ç«¯åˆ°ç«¯äº¤æ˜“æµç¨‹æµ‹è¯•"
        start_time = time.time()
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹ {test_name}")
            
            # ç”Ÿæˆæµ‹è¯•ç­–ç•¥ID
            strategy_id = f"test_strategy_{uuid.uuid4().hex[:8]}"
            
            # æ¨¡æ‹Ÿç­–ç•¥åŠ è½½
            assert self.trading_engine is not None
            load_success = await self.trading_engine.strategy_manager.load_strategy(
                "src/strategies/minimal_strategy.py",
                strategy_id,
                {
                    "symbol": "FG509",
                    "exchange": "CZCE",
                    "volume": 1,
                    "order_interval": 5,
                    "max_orders": 10
                }
            )
            
            if not load_success:
                raise Exception("ç­–ç•¥åŠ è½½å¤±è´¥")
            
            # å¯åŠ¨ç­–ç•¥
            start_success = await self.trading_engine.strategy_manager.start_strategy(strategy_id)
            if not start_success:
                raise Exception("ç­–ç•¥å¯åŠ¨å¤±è´¥")
            
            # æ¨¡æ‹Ÿå¸‚åœºæ•°æ®
            await self._send_mock_market_data("FG509", 100)
            
            # æ¨¡æ‹Ÿä¸‹å•
            order_request = OrderRequest(
                symbol="FG509",
                exchange=Exchange.CZCE,
                direction=Direction.LONG,
                type=OrderType.LIMIT,
                volume=1,
                price=4500.0
            )
            
            order_id = await self.trading_engine.order_manager.place_order(order_request, strategy_id)
            if not order_id:
                raise Exception("ä¸‹å•å¤±è´¥")
            
            # ç­‰å¾…è®¢å•å¤„ç†
            await asyncio.sleep(1)
            
            # åœæ­¢ç­–ç•¥
            await self.trading_engine.strategy_manager.stop_strategy(strategy_id)
            
            duration = time.time() - start_time
            
            # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
            assert self.performance_monitor is not None
            performance_metrics = self.performance_monitor.get_performance_summary(strategy_id) if hasattr(self.performance_monitor, 'get_performance_summary') else {}
            system_metrics = self.performance_monitor.get_system_metrics() if hasattr(self.performance_monitor, 'get_system_metrics') else {}
            
            result = TestResult(
                test_name=test_name,
                success=True,
                duration=duration,
                metrics={
                    "strategy_performance": performance_metrics,
                    "system_performance": system_metrics,
                    "order_id": order_id
                }
            )
            
            logger.info(f"âœ… {test_name} å®Œæˆï¼Œè€—æ—¶: {duration:.3f}ç§’")
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"âŒ {test_name} å¤±è´¥: {e}")
            
            return TestResult(
                test_name=test_name,
                success=False,
                duration=duration,
                error_message=str(e)
            )
    
    async def run_stress_test(self, concurrent_operations: int = 100, duration_seconds: int = 30) -> TestResult:
        """å‹åŠ›æµ‹è¯• - å¹¶å‘æ“ä½œæµ‹è¯•"""
        test_name = f"å‹åŠ›æµ‹è¯•_{concurrent_operations}å¹¶å‘_{duration_seconds}ç§’"
        start_time = time.time()
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹ {test_name}")
            
            # è®°å½•ç³»ç»Ÿåˆå§‹çŠ¶æ€
            initial_memory = psutil.virtual_memory().used / 1024 / 1024
            initial_cpu = psutil.cpu_percent()
            
            # å¹¶å‘ä»»åŠ¡åˆ—è¡¨
            tasks = []
            results = []
            
            # åˆ›å»ºå¹¶å‘ä»»åŠ¡
            for i in range(concurrent_operations):
                task = asyncio.create_task(self._concurrent_trading_operation(f"stress_test_{i}"))
                tasks.append(task)
            
            # è¿è¡ŒæŒ‡å®šæ—¶é—´
            test_end_time = start_time + duration_seconds
            completed_count = 0
            error_count = 0
            
            while time.time() < test_end_time and tasks:
                done, pending = await asyncio.wait(tasks, timeout=1.0, return_when=asyncio.FIRST_COMPLETED)
                
                for task in done:
                    tasks.remove(task)
                    try:
                        result = await task
                        results.append(result)
                        completed_count += 1
                    except Exception as e:
                        error_count += 1
                        logger.warning(f"å¹¶å‘ä»»åŠ¡å¤±è´¥: {e}")
                    
                    # åˆ›å»ºæ–°ä»»åŠ¡è¡¥å……
                    if time.time() < test_end_time:
                        new_task = asyncio.create_task(
                            self._concurrent_trading_operation(f"stress_test_{completed_count + error_count}")
                        )
                        tasks.append(new_task)
            
            # å–æ¶ˆå‰©ä½™ä»»åŠ¡
            for task in tasks:
                task.cancel()
            
            # è®°å½•ç³»ç»Ÿæœ€ç»ˆçŠ¶æ€
            final_memory = psutil.virtual_memory().used / 1024 / 1024
            final_cpu = psutil.cpu_percent()
            
            duration = time.time() - start_time
            
            # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
            operations_per_second = completed_count / duration
            memory_usage_increase = final_memory - initial_memory
            
            result = TestResult(
                test_name=test_name,
                success=True,
                duration=duration,
                metrics={
                    "completed_operations": completed_count,
                    "error_operations": error_count,
                    "operations_per_second": operations_per_second,
                    "memory_usage_increase_mb": memory_usage_increase,
                    "initial_cpu_percent": initial_cpu,
                    "final_cpu_percent": final_cpu,
                    "success_rate": completed_count / (completed_count + error_count) if (completed_count + error_count) > 0 else 0
                }
            )
            
            logger.info(f"âœ… {test_name} å®Œæˆ")
            logger.info(f"   - å®Œæˆæ“ä½œ: {completed_count}")
            logger.info(f"   - é”™è¯¯æ“ä½œ: {error_count}")
            logger.info(f"   - æ¯ç§’æ“ä½œæ•°: {operations_per_second:.2f}")
            logger.info(f"   - å†…å­˜å¢é•¿: {memory_usage_increase:.2f}MB")
            
            return result
            
        except Exception as e:
            duration = time.time() - start_time
            logger.error(f"âŒ {test_name} å¤±è´¥: {e}")
            
            return TestResult(
                test_name=test_name,
                success=False,
                duration=duration,
                error_message=str(e)
            )
    
    async def run_latency_benchmark(self, operation_count: int = 1000) -> BenchmarkResult:
        """å»¶è¿ŸåŸºå‡†æµ‹è¯•"""
        operation = "è®¢å•å¤„ç†å»¶è¿Ÿ"
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹å»¶è¿ŸåŸºå‡†æµ‹è¯• - {operation_count} æ¬¡æ“ä½œ")
            
            latencies = []
            error_count = 0
            start_time = time.time()
            
            for i in range(operation_count):
                try:
                    operation_start = time.time()
                    
                    # æ‰§è¡Œå•æ¬¡æ“ä½œ
                    await self._single_order_operation(f"benchmark_{i}")
                    
                    operation_end = time.time()
                    latency_ms = (operation_end - operation_start) * 1000
                    latencies.append(latency_ms)
                    
                except Exception as e:
                    error_count += 1
                    logger.debug(f"åŸºå‡†æµ‹è¯•æ“ä½œå¤±è´¥: {e}")
            
            total_duration = time.time() - start_time
            successful_operations = len(latencies)
            
            if successful_operations == 0:
                raise Exception("æ‰€æœ‰åŸºå‡†æµ‹è¯•æ“ä½œéƒ½å¤±è´¥äº†")
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            avg_latency = statistics.mean(latencies)
            p95_latency = statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else max(latencies)
            p99_latency = statistics.quantiles(latencies, n=100)[98] if len(latencies) >= 100 else max(latencies)
            
            result = BenchmarkResult(
                operation=operation,
                total_operations=operation_count,
                duration=total_duration,
                operations_per_second=successful_operations / total_duration,
                avg_latency_ms=avg_latency,
                p95_latency_ms=p95_latency,
                p99_latency_ms=p99_latency,
                error_count=error_count,
                success_rate=successful_operations / operation_count
            )
            
            logger.info(f"âœ… å»¶è¿ŸåŸºå‡†æµ‹è¯•å®Œæˆ")
            logger.info(f"   - æˆåŠŸæ“ä½œ: {successful_operations}/{operation_count}")
            logger.info(f"   - å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ms")
            logger.info(f"   - P95å»¶è¿Ÿ: {p95_latency:.2f}ms")
            logger.info(f"   - P99å»¶è¿Ÿ: {p99_latency:.2f}ms")
            logger.info(f"   - æˆåŠŸç‡: {result.success_rate:.2%}")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ å»¶è¿ŸåŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            raise
    
    async def run_throughput_benchmark(self, duration_seconds: int = 60) -> BenchmarkResult:
        """ååé‡åŸºå‡†æµ‹è¯•"""
        operation = "ç³»ç»Ÿååé‡"
        
        try:
            logger.info(f"ğŸš€ å¼€å§‹ååé‡åŸºå‡†æµ‹è¯• - {duration_seconds} ç§’")
            
            start_time = time.time()
            end_time = start_time + duration_seconds
            
            completed_operations = 0
            error_count = 0
            latencies = []
            
            while time.time() < end_time:
                try:
                    operation_start = time.time()
                    
                    # æ‰§è¡Œæ“ä½œ
                    await self._high_frequency_operation(completed_operations)
                    
                    operation_end = time.time()
                    latency_ms = (operation_end - operation_start) * 1000
                    latencies.append(latency_ms)
                    completed_operations += 1
                    
                except Exception as e:
                    error_count += 1
                    logger.debug(f"ååé‡æµ‹è¯•æ“ä½œå¤±è´¥: {e}")
            
            total_duration = time.time() - start_time
            
            if completed_operations == 0:
                raise Exception("ååé‡æµ‹è¯•ä¸­æ²¡æœ‰æˆåŠŸçš„æ“ä½œ")
            
            # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
            avg_latency = statistics.mean(latencies)
            p95_latency = statistics.quantiles(latencies, n=20)[18] if len(latencies) >= 20 else max(latencies)
            p99_latency = statistics.quantiles(latencies, n=100)[98] if len(latencies) >= 100 else max(latencies)
            
            result = BenchmarkResult(
                operation=operation,
                total_operations=completed_operations + error_count,
                duration=total_duration,
                operations_per_second=completed_operations / total_duration,
                avg_latency_ms=avg_latency,
                p95_latency_ms=p95_latency,
                p99_latency_ms=p99_latency,
                error_count=error_count,
                success_rate=completed_operations / (completed_operations + error_count)
            )
            
            logger.info(f"âœ… ååé‡åŸºå‡†æµ‹è¯•å®Œæˆ")
            logger.info(f"   - æ€»æ“ä½œæ•°: {completed_operations}")
            logger.info(f"   - é”™è¯¯æ•°: {error_count}")
            logger.info(f"   - ååé‡: {result.operations_per_second:.2f} ops/s")
            logger.info(f"   - å¹³å‡å»¶è¿Ÿ: {avg_latency:.2f}ms")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ ååé‡åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
            raise
    
    # è¾…åŠ©æ–¹æ³•
    async def _send_mock_market_data(self, symbol: str, count: int) -> None:
        """å‘é€æ¨¡æ‹Ÿå¸‚åœºæ•°æ®"""
        base_price = 4500.0
        
        for i in range(count):
            tick_data = TickData(
                symbol=symbol,
                exchange=Exchange.CZCE,
                datetime=datetime.now(),
                last_price=base_price + (i % 10) * 0.2,
                volume=100 + i,
                turnover=(base_price + (i % 10) * 0.2) * (100 + i),
                open_interest=1000 + i,
                gateway_name="send_mock_market_data"
            )
            
            assert self.event_bus is not None
            self.event_bus.publish(Event("market.tick", tick_data))
            await asyncio.sleep(0.001)  # 1msé—´éš”
    
    async def _concurrent_trading_operation(self, operation_id: str) -> str:
        """å¹¶å‘äº¤æ˜“æ“ä½œ"""
        # æ¨¡æ‹Ÿç­–ç•¥å¤„ç†
        await asyncio.sleep(0.001)
        
        # å‘å¸ƒæ¨¡æ‹Ÿäº‹ä»¶
        assert self.event_bus is not None
        self.event_bus.publish(create_trading_event(
            "strategy.signal",
            {
                "action": "place_order",
                "strategy_id": operation_id,
                "order_request": OrderRequest(
                    symbol="FG509",
                    exchange=Exchange.CZCE,
                    direction=Direction.LONG,
                    type=OrderType.LIMIT,
                    volume=1,
                    price=4500.0
                )
            },
            operation_id
        ))
        
        # æ¨¡æ‹Ÿå¤„ç†å»¶è¿Ÿ
        await asyncio.sleep(0.005)
        
        return operation_id
    
    async def _single_order_operation(self, operation_id: str) -> None:
        """å•æ¬¡è®¢å•æ“ä½œ"""
        order_request = OrderRequest(
            symbol="FG509",
            exchange=Exchange.CZCE,
            direction=Direction.LONG,
            type=OrderType.LIMIT,
            volume=1,
            price=4500.0
        )
        
        # å‘å¸ƒè®¢å•äº‹ä»¶
        assert self.event_bus is not None
        self.event_bus.publish(create_trading_event(
            "strategy.signal",
            {
                "action": "place_order",
                "strategy_id": operation_id,
                "order_request": order_request
            },
            operation_id
        ))
        
        # ç­‰å¾…å¤„ç†
        await asyncio.sleep(0.001)
    
    async def _high_frequency_operation(self, operation_id: int) -> None:
        """é«˜é¢‘æ“ä½œ"""
        # å‘å¸ƒé«˜é¢‘tickäº‹ä»¶
        tick_data = TickData(
            symbol="FG509",
            exchange=Exchange.CZCE,
            datetime=datetime.now(),
            last_price=4500.0 + (operation_id % 100) * 0.1,
            volume=100,
            turnover=450000.0,
            open_interest=1000,
            gateway_name="high_frequency_operation"
        )
        
        assert self.event_bus is not None
        self.event_bus.publish(Event("market.tick", tick_data))
        
        # å¾®å°å»¶è¿Ÿ
        await asyncio.sleep(0.0001)
    
    def generate_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        return {
            "test_summary": {
                "total_tests": len(self.test_results),
                "passed_tests": sum(1 for r in self.test_results if r.success),
                "failed_tests": sum(1 for r in self.test_results if not r.success),
                "total_duration": sum(r.duration for r in self.test_results)
            },
            "test_results": [
                {
                    "name": r.test_name,
                    "success": r.success,
                    "duration": r.duration,
                    "metrics": r.metrics,
                    "error": r.error_message
                }
                for r in self.test_results
            ],
            "benchmark_results": [
                {
                    "operation": b.operation,
                    "total_operations": b.total_operations,
                    "duration": b.duration,
                    "ops_per_second": b.operations_per_second,
                    "avg_latency_ms": b.avg_latency_ms,
                    "p95_latency_ms": b.p95_latency_ms,
                    "p99_latency_ms": b.p99_latency_ms,
                    "error_count": b.error_count,
                    "success_rate": b.success_rate
                }
                for b in self.benchmark_results
            ]
        }


# æµ‹è¯•ç”¨ä¾‹
class TestPerformanceFramework:
    """æ€§èƒ½æµ‹è¯•ç”¨ä¾‹é›†åˆ"""
    
    @pytest.fixture
    async def performance_framework(self) -> Any:
        """æ€§èƒ½æµ‹è¯•æ¡†æ¶å¤¹å…·"""
        framework = PerformanceTestFramework()
        setup_success = await framework.setup_test_environment()
        if not setup_success:
            pytest.skip("æµ‹è¯•ç¯å¢ƒè®¾ç½®å¤±è´¥")
        
        yield framework
        
        await framework.teardown_test_environment()
    
    @pytest.mark.asyncio
    async def test_end_to_end_performance(self, performance_framework: Any) -> None:
        """æµ‹è¯•ç«¯åˆ°ç«¯æ€§èƒ½"""
        result = await performance_framework.run_end_to_end_test()
        performance_framework.test_results.append(result)
        
        assert result.success, f"ç«¯åˆ°ç«¯æµ‹è¯•å¤±è´¥: {result.error_message}"
        assert result.duration < 10.0, f"ç«¯åˆ°ç«¯æµ‹è¯•è€—æ—¶è¿‡é•¿: {result.duration}ç§’"
    
    @pytest.mark.asyncio
    async def test_stress_performance(self, performance_framework: Any) -> None:
        """æµ‹è¯•å‹åŠ›æ€§èƒ½"""
        result = await performance_framework.run_stress_test(concurrent_operations=50, duration_seconds=10)
        performance_framework.test_results.append(result)
        
        assert result.success, f"å‹åŠ›æµ‹è¯•å¤±è´¥: {result.error_message}"
        assert result.metrics.get("success_rate", 0) > 0.8, "å‹åŠ›æµ‹è¯•æˆåŠŸç‡è¿‡ä½"
    
    @pytest.mark.asyncio
    async def test_latency_benchmark(self, performance_framework):
        """æµ‹è¯•å»¶è¿ŸåŸºå‡†"""
        result = await performance_framework.run_latency_benchmark(operation_count=100)
        performance_framework.benchmark_results.append(result)
        
        assert result.success_rate > 0.9, "å»¶è¿ŸåŸºå‡†æµ‹è¯•æˆåŠŸç‡è¿‡ä½"
        assert result.avg_latency_ms < 50, f"å¹³å‡å»¶è¿Ÿè¿‡é«˜: {result.avg_latency_ms}ms"
    
    @pytest.mark.asyncio
    async def test_throughput_benchmark(self, performance_framework):
        """æµ‹è¯•ååé‡åŸºå‡†"""
        result = await performance_framework.run_throughput_benchmark(duration_seconds=10)
        performance_framework.benchmark_results.append(result)
        
        assert result.operations_per_second > 100, f"ååé‡è¿‡ä½: {result.operations_per_second} ops/s"
        assert result.success_rate > 0.8, "ååé‡æµ‹è¯•æˆåŠŸç‡è¿‡ä½"


if __name__ == "__main__":
    async def main() -> None:
        """ä¸»æµ‹è¯•å‡½æ•°"""
        framework = PerformanceTestFramework()
        
        try:
            # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
            if not await framework.setup_test_environment():
                logger.error("æµ‹è¯•ç¯å¢ƒè®¾ç½®å¤±è´¥")
                return
            
            # è¿è¡Œæµ‹è¯•å¥—ä»¶
            logger.info("ğŸš€ å¼€å§‹æ€§èƒ½æµ‹è¯•å¥—ä»¶")
            
            # ç«¯åˆ°ç«¯æµ‹è¯•
            e2e_result = await framework.run_end_to_end_test()
            framework.test_results.append(e2e_result)
            
            # å‹åŠ›æµ‹è¯•
            stress_result = await framework.run_stress_test(concurrent_operations=20, duration_seconds=10)
            framework.test_results.append(stress_result)
            
            # å»¶è¿ŸåŸºå‡†æµ‹è¯•
            latency_benchmark = await framework.run_latency_benchmark(operation_count=500)
            framework.benchmark_results.append(latency_benchmark)
            
            # ååé‡åŸºå‡†æµ‹è¯•
            throughput_benchmark = await framework.run_throughput_benchmark(duration_seconds=15)
            framework.benchmark_results.append(throughput_benchmark)
            
            # ç”ŸæˆæŠ¥å‘Š
            report = framework.generate_performance_report()
            
            logger.info("ğŸ“Š æ€§èƒ½æµ‹è¯•æŠ¥å‘Š:")
            logger.info(f"æ€»æµ‹è¯•æ•°: {report['test_summary']['total_tests']}")
            logger.info(f"é€šè¿‡æµ‹è¯•: {report['test_summary']['passed_tests']}")
            logger.info(f"å¤±è´¥æµ‹è¯•: {report['test_summary']['failed_tests']}")
            logger.info(f"æ€»è€—æ—¶: {report['test_summary']['total_duration']:.3f}ç§’")
            
            print("\n" + "="*60)
            print("ğŸ¯ Homalosæ€§èƒ½æµ‹è¯•ç»“æœæ±‡æ€»")
            print("="*60)
            
            for result in framework.test_results:
                status = "âœ… é€šè¿‡" if result.success else "âŒ å¤±è´¥"
                print(f"{status} {result.test_name} - {result.duration:.3f}ç§’")
                if not result.success:
                    print(f"   é”™è¯¯: {result.error_message}")
            
            print("\nåŸºå‡†æµ‹è¯•ç»“æœ:")
            for benchmark in framework.benchmark_results:
                print(f"ğŸ“ˆ {benchmark.operation}:")
                print(f"   - ååé‡: {benchmark.operations_per_second:.2f} ops/s")
                print(f"   - å¹³å‡å»¶è¿Ÿ: {benchmark.avg_latency_ms:.2f}ms")
                print(f"   - P95å»¶è¿Ÿ: {benchmark.p95_latency_ms:.2f}ms")
                print(f"   - æˆåŠŸç‡: {benchmark.success_rate:.2%}")
            
            print("="*60)
            
        finally:
            await framework.teardown_test_environment()
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main()) 